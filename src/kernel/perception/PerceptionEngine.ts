import { generateText } from '../llm/gateway';
import { traceSpan } from '../observability';

/**
 * PERCEPTION ENGINE (The Eyes)
 * 
 * Replaces fragile CSS-based "Vendor Detectors" (ChatGPT/Claude identifiers)
 * with semantic analysis of the content itself.
 * 
 * Capability:
 * - Detects "Synthetic" vs "Organic" text signatures.
 * - Classifies content based on semantic density, not HTML tags.
 */
export class PerceptionEngine {

    /**
     * Analyzes a text block to determine if it's likely AI-generated.
     * Uses a lightweight heuristic + RLM verification.
     */
    static async analyzeOrigin(text: string): Promise<{ origin: 'ai' | 'human'; confidence: number; provider?: string | undefined }> {
        return traceSpan('perception.analyze_origin', { textLength: text.length }, async () => {

            // 1. Fast Entropy Heuristic (Client-side pre-check)
            // AI text tends to have "perfect" grammar and median sentence length.
            const entropyScore = this.calculateEntropyHeuristic(text);

            // If text is short or ambiguous, use the RLM (Gateway)
            if (entropyScore.confidence < 0.8) {
                const rlmResult = await this.askRLM(text);
                return rlmResult;
            }

            return {
                origin: entropyScore.origin,
                confidence: entropyScore.confidence
            };
        });
    }

    /**
     * Calculates textual entropy (variation in sentence length and vocabulary).
     */
    private static calculateEntropyHeuristic(text: string): { origin: 'ai' | 'human'; confidence: number } {
        const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);
        if (sentences.length === 0) return { origin: 'human', confidence: 0.5 };

        // Metric 1: Sentence Length Variance (Human is choppier)
        const lengths = sentences.map(s => s.trim().split(/\s+/).length);
        const mean = lengths.reduce((a, b) => a + b, 0) / lengths.length;
        const variance = lengths.reduce((a, b) => a + Math.pow(b - mean, 2), 0) / lengths.length;

        // AI tends to be very consistent (Low Variance)
        // Humans are erratic (High Variance)
        const isLowVariance = variance < 10;

        // Metric 2: "Transition Words" density (AI loves 'Therefore', 'However', 'Furthermore')
        const aiMarkers = ['therefore', 'furthermore', 'consequently', 'in summary', 'secondly'];
        const lowerText = text.toLowerCase();
        const markerCount = aiMarkers.filter(m => lowerText.includes(m)).length;
        const isHighMarkerDensity = (markerCount / sentences.length) > 0.3;

        if (isLowVariance && isHighMarkerDensity) {
            return { origin: 'ai', confidence: 0.75 };
        }

        return { origin: 'human', confidence: 0.6 };
    }

    /**
     * Zero-Shot classification via main Gateway.
     */
    private static async askRLM(text: string): Promise<{ origin: 'ai' | 'human'; confidence: number; provider?: string }> {
        const prompt = `
        Analyze the following text sample. 
        Determine if it was chemically generated by an AI (LLM) or written organically by a Human.
        
        Text Sample: """${text.substring(0, 500)}..."""
        
        Output JSON: { "origin": "ai" | "human", "confidence": 0-1, "reasoning": "string" }
        `;

        try {
            const response = await generateText(prompt, "Origin Analysis", "EFFICIENCY");
            const result = JSON.parse(response.content);
            return {
                origin: result.origin,
                confidence: result.confidence,
                provider: result.origin === 'ai' ? 'unknown-llm' : undefined
            };
        } catch (e) {
            console.warn('[PerceptionEngine] RLM classification failed, defaulting to heuristic.');
            return { origin: 'ai', confidence: 0.5, provider: undefined }; // Fail-safe default
        }
    }
}
